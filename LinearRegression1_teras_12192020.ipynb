{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression_teras_12192020.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdfmH7TKN2JKL2QuD9+p6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swimbeginner/TensorFlow2/blob/main/LinearRegression1_teras_12192020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9202ybL4gm"
      },
      "source": [
        "#Linear Regression with Teras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYUflViCL-ab"
      },
      "source": [
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UEyXtENjMUTh",
        "outputId": "5abf531a-a415-4e3f-fe0d-46290a60da8b"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N85V06kMk4R"
      },
      "source": [
        "# Define functions that build and train a model\r\n",
        "\r\n",
        "The following code defines two functions:\r\n",
        "\r\n",
        "  * `build_model(my_learning_rate)`, which builds an empty model.\r\n",
        "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass. \r\n",
        "\r\n",
        "Since you don't need to understand model building code right now, we've hidden this code cell.  You may optionally double-click the headline to explore this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahHWjYogMXAp"
      },
      "source": [
        "my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\r\n",
        "my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC3QLZ9gM4x5"
      },
      "source": [
        "learning_rate=0.1\r\n",
        "epochs=10\r\n",
        "my_batch_size=10\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "romiTuHCPO5x"
      },
      "source": [
        "## psuedo-code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLo66b5BNv8v",
        "outputId": "75ca08be-4b04-40e5-8acd-4cb95a9442ce"
      },
      "source": [
        "my_model = build_model(learning_rate)\r\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \r\n",
        "                                                        my_label, epochs,\r\n",
        "                                                        my_batch_size)\r\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\r\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `tf.build_model` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfwyhgKPt1H"
      },
      "source": [
        "## Task 1: Examine the graphs\r\n",
        "\r\n",
        "Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots.  Does it?  Probably not.\r\n",
        "\r\n",
        "A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train.  That said, unless you are an extremely lucky person, the red line probably *doesn't* align nicely with the blue dots.  \r\n",
        "\r\n",
        "Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wlDbftsPrhv"
      },
      "source": [
        "## Task 2: Increase the number of epochs\r\n",
        "\r\n",
        "Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has [converged](http://developers.google.com/machine-learning/glossary/#convergence).\r\n",
        "\r\n",
        "In Task 1, the training loss did not converge. One possible solution is to train for more epochs.  Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value.\r\n",
        "\r\n",
        "Examine the loss curve. Does the model converge?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_mUXnjdPx1e"
      },
      "source": [
        "## Task 3: Increase the learning rate\r\n",
        "\r\n",
        "In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQhk4sg5P12n"
      },
      "source": [
        "## Task 4: Find the ideal combination of epochs and learning rate\r\n",
        "\r\n",
        "Assign values to the following two hyperparameters to make training converge as efficiently as possible: \r\n",
        "\r\n",
        "*  learning_rate\r\n",
        "*  epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "773kmTU_P-nu"
      },
      "source": [
        "## Task 5: Adjust the batch size\r\n",
        "\r\n",
        "The system recalculates the model's loss value and adjusts the model's weights and bias after each **iteration**.  Each iteration is the span in which the system processes one batch. For example, if the **batch size** is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples.  \r\n",
        "\r\n",
        "One **epoch** spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations.  \r\n",
        "\r\n",
        "It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. \r\n",
        "\r\n",
        "Experiment with `batch_size` in the following code cell. What's the smallest integer you can set for `batch_size` and still have the model converge in a hundred epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiN3SM1OPH7Z"
      },
      "source": [
        "## Summary of hyperparameter tuning\r\n",
        "\r\n",
        "Most machine learning problems require a lot of hyperparameter tuning.  Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly.  You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb:\r\n",
        "\r\n",
        " * Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. \r\n",
        " * If the training loss does not converge, train for more epochs.\r\n",
        " * If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\r\n",
        " * If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\r\n",
        " * Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\r\n",
        " * Setting the batch size to a *very* small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\r\n",
        " * For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. \r\n",
        "\r\n",
        "Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify."
      ]
    }
  ]
}